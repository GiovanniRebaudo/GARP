mse(auto.test$mpg, yhat.test)
library(dplyr)
Auto_new = Auto %>% mutate(hp = I(horsepower^2))
auto.new.train = Auto_new[train, ]
auto.new.test = Auto_new[-train, ]
fitq = lm(mpg ~ horsepower + hp, data = auto.new.train)
yhatq.train = predict(fitq)
yhatq.test = predict(fitq, newdata = auto.new.test)
mse(auto.new.train$mpg, yhatq.train)
mse(auto.new.test$mpg, yhatq.test)
set.seed(111111)
n = dim(Auto)[1]
mse.train = c(); mse.test = c()
mseq.train = c(); mseq.test = c()
for (i in 1:10){
n.train = floor(.75*n)    # 75% training
train = sample(1:n, size = n.train, rep = F)
auto.train = Auto[train, ]
auto.test = Auto[-train, ]
fit = lm(mpg ~ horsepower, data = auto.train)
yhat.train = predict(fit)
yhat.test = predict(fit, newdata = auto.test)
mse.train[i] = mse(auto.train$mpg, yhat.train)
mse.test[i] = mse(auto.test$mpg, yhat.test)
auto.new.train = Auto_new[train, ]
auto.new.test = Auto_new[-train, ]
fitq = lm(mpg ~ horsepower + hp, data = auto.new.train)
yhatq.train = predict(fitq)
yhatq.test = predict(fitq, newdata = auto.new.test)
mseq.train[i] = mse(auto.new.train$mpg, yhatq.train)
mseq.test[i] = mse(auto.new.test$mpg, yhatq.test)
}
boxplot(as.data.frame(cbind(mse.train, mse.test, mseq.train, mseq.test), pch = 16), main = " ", ylab = "MSE", names=c("LM train", "LM test", "LM w/Q train", "LM w/Q test"))
library(boot)
fit = glm(mpg ~ horsepower, data = Auto, family = gaussian)
cv.glm(Auto, fit, cost = mse, K = 10)$delta
fit_new = glm(mpg ~ horsepower + hp, data = Auto_new,
family = gaussian)
cv.glm(Auto_new, fit_new, cost = mse, K = 10)$delta
install.packages(c("dplyr", "gtable", "hms", "MASS", "pillar", "survival", "vctrs"))
2065.03/3*4
###
library(ISLR2)
###
library(ISLR2)
###
library(ISLR2)
###
library(ISLR2)
set.seed(1)
train <- sample(392, 196)
###
lm.fit <- lm(mpg ~ horsepower, data = Auto, subset = train)
###
attach(Auto)
?sample
train
str(train)
summary(Auto)
###
lm.fit <- lm(mpg ~ horsepower, data = Auto, subset = train)
###
attach(Auto)
mean((mpg - predict(lm.fit, Auto))[-train]^2)
poly(horsepower, 2)
mean((mpg - predict(lm.fit3, Auto))[-train]^2)
###
lm.fit2 <- lm(mpg ~ poly(horsepower, 2), data = Auto,
subset = train)
mean((mpg - predict(lm.fit2, Auto))[-train]^2)
lm.fit3 <- lm(mpg ~ poly(horsepower, 3), data = Auto,
subset = train)
mean((mpg - predict(lm.fit3, Auto))[-train]^2)
###
set.seed(2)
train <- sample(392, 196)
lm.fit <- lm(mpg ~ horsepower, subset = train)
mean((mpg - predict(lm.fit, Auto))[-train]^2)
lm.fit2 <- lm(mpg ~ poly(horsepower, 2), data = Auto,
subset = train)
mean((mpg - predict(lm.fit2, Auto))[-train]^2)
lm.fit3 <- lm(mpg ~ poly(horsepower, 3), data = Auto,
subset = train)
mean((mpg - predict(lm.fit3, Auto))[-train]^2)
summary(mgp)
summary(mgp)
summary(Auto$mgp)
mgp
###
attach(Auto)
mgp
summary(Auto)
frequency(Auto$mpg)
table(Auto$mpg)
###
glm.fit <- glm(mpg ~ horsepower, data = Auto)
coef(glm.fit)
###
lm.fit <- lm(mpg ~ horsepower, data = Auto)
coef(lm.fit)
###
library(boot)
glm.fit <- glm(mpg ~ horsepower, data = Auto)
cv.err
glm.fit <- glm(mpg ~ horsepower, data = Auto)
cv.err <- cv.glm(Auto, glm.fit) #pretty slow (it doesn't use the shortcut)
cv.err$delta
cv.err$delta
cv.err <- cv.glm(Auto, glm.fit)
# plot it
plot(cv.error,type="b")
for (i in 1:10) {
glm.fit <- glm(mpg ~ poly(horsepower, i), data = Auto)
cv.error[i] <- cv.glm(Auto, glm.fit)$delta[1]
}
# Lab: Cross-Validation and the Bootstrap
## The Validation Set Approach
###
library(ISLR2)
set.seed(1)
train <- sample(392, 196)
###
lm.fit <- lm(mpg ~ horsepower, data = Auto, subset = train)
###
attach(Auto)
mean((mpg - predict(lm.fit, Auto))[-train]^2)
###
lm.fit2 <- lm(mpg ~ poly(horsepower, 2), data = Auto,
subset = train)
mean((mpg - predict(lm.fit2, Auto))[-train]^2)
lm.fit3 <- lm(mpg ~ poly(horsepower, 3), data = Auto,
subset = train)
mean((mpg - predict(lm.fit3, Auto))[-train]^2)
###
set.seed(2)
train <- sample(392, 196)
lm.fit <- lm(mpg ~ horsepower, subset = train)
mean((mpg - predict(lm.fit, Auto))[-train]^2)
lm.fit2 <- lm(mpg ~ poly(horsepower, 2), data = Auto,
subset = train)
mean((mpg - predict(lm.fit2, Auto))[-train]^2)
lm.fit3 <- lm(mpg ~ poly(horsepower, 3), data = Auto,
subset = train)
mean((mpg - predict(lm.fit3, Auto))[-train]^2)
## Leave-One-Out Cross-Validation
###
glm.fit <- glm(mpg ~ horsepower, data = Auto)
coef(glm.fit)
###
lm.fit <- lm(mpg ~ horsepower, data = Auto)
coef(lm.fit)
###
library(boot)
glm.fit <- glm(mpg ~ horsepower, data = Auto)
cv.err <- cv.glm(Auto, glm.fit)
cv.err$delta
###
cv.error <- rep(0, 10)
for (i in 1:10) {
glm.fit <- glm(mpg ~ poly(horsepower, i), data = Auto)
cv.error[i] <- cv.glm(Auto, glm.fit)$delta[1]
}
cv.error
2500/8
### Lets write a simple function to compute the short cut formula
loocv=function(fit){
h=lm.influence(fit)$h
mean((residuals(fit)/(1-h))^2)
}
# Lab: Cross-Validation and the Bootstrap
## The Validation Set Approach
###
library(ISLR2)
set.seed(1)
train <- sample(392, 196)
###
lm.fit <- lm(mpg ~ horsepower, data = Auto, subset = train)
###
attach(Auto)
mean((mpg - predict(lm.fit, Auto))[-train]^2)
###
lm.fit2 <- lm(mpg ~ poly(horsepower, 2), data = Auto,
subset = train)
mean((mpg - predict(lm.fit2, Auto))[-train]^2)
lm.fit3 <- lm(mpg ~ poly(horsepower, 3), data = Auto,
subset = train)
mean((mpg - predict(lm.fit3, Auto))[-train]^2)
###
set.seed(2)
train <- sample(392, 196)
lm.fit <- lm(mpg ~ horsepower, subset = train)
mean((mpg - predict(lm.fit, Auto))[-train]^2)
lm.fit2 <- lm(mpg ~ poly(horsepower, 2), data = Auto,
subset = train)
mean((mpg - predict(lm.fit2, Auto))[-train]^2)
lm.fit3 <- lm(mpg ~ poly(horsepower, 3), data = Auto,
subset = train)
mean((mpg - predict(lm.fit3, Auto))[-train]^2)
## Leave-One-Out Cross-Validation
###
glm.fit <- glm(mpg ~ horsepower, data = Auto)
coef(glm.fit)
###
lm.fit <- lm(mpg ~ horsepower, data = Auto)
coef(lm.fit)
###
library(boot)
glm.fit <- glm(mpg ~ horsepower, data = Auto)
cv.err <- cv.glm(Auto, glm.fit)
cv.err$delta
### Lets write a simple function to compute the short cut formula
loocv=function(fit){
h=lm.influence(fit)$h
mean((residuals(fit)/(1-h))^2)
}
###
cv.error <- rep(0, 10)
for (i in 1:10) {
glm.fit <- glm(mpg ~ poly(horsepower, i), data = Auto)
cv.error[i] <- cv.glm(Auto, glm.fit)$delta[1]
}
cv.error
# plot it
plot(cv.error,type="b")
loocv(glm:fit)
loocv(glm.fit)
loocv(glm.fit)
cv.glm(Auto, glm.fit)
loocv(glm.fit)
cv.glm(Auto, glm.fit)$delta[1]
###
cv.error <- rep(0, 10)
for (i in 1:10) {
glm.fit <- glm(mpg ~ poly(horsepower, i), data = Auto)
cv.error[i] <- loocv(glm.fit)
}
cv.error
# plot it
plot(cv.error,type="b")
for (i in 1:10) {
glm.fit <- glm(mpg ~ poly(horsepower, i), data = Auto)
cv.error[i] <- loocv(glm.fit)
}
cv.error
###
cv.error <- rep(0, 10)
for (d in 1:10) {
glm.fit <- glm(mpg ~ poly(horsepower, d), data = Auto)
cv.error[d] <- loocv(glm.fit)
}
cv.error
# plot it
plot(cv.error,type="b")
## $k$-Fold Cross-Validation
###
set.seed(17)
###
cv.error <- rep(0, 10)
degree <- 1:10
for (d in degree) {
glm.fit <- glm(mpg ~ poly(horsepower, d), data = Auto)
cv.error[d] <- loocv(glm.fit)
}
cv.error
# plot it
plot(degree,cv.error,type="b")
## $k$-Fold Cross-Validation
###
set.seed(17)
cv.error.10 <- rep(0, 10)
for (i in 1:10) {
glm.fit <- glm(mpg ~ poly(horsepower, i), data = Auto)
cv.error.10[i] <- cv.glm(Auto, glm.fit, K = 10)$delta[1]
}
cv.error.10
## $k$-Fold Cross-Validation
###
set.seed(17)
cv.error.10 <- rep(0, 10)
for (d in degree) {
glm.fit <- glm(mpg ~ poly(horsepower, d), data = Auto)
cv.error.10[d] <- cv.glm(Auto, glm.fit, K = 10)$delta[1]
}
cv.error.10
cv.error.10 <- rep(0, 10)
for (d in degree) {
glm.fit <- glm(mpg ~ poly(horsepower, d), data = Auto)
cv.error.10[d] <- cv.glm(Auto, glm.fit, K = 10)$delta[1]
}
cv.error.10
lines(degree, cv.error.10, type="b",col="red")
# See bias-correction
cv.glm(Auto, glm.fit, K = 10)$delta
###
alpha.fn <- function(data, index) {
X <- data$X[index]
Y <- data$Y[index]
(var(Y) - cov(X, Y)) / (var(X) + var(Y) - 2 * cov(X, Y))
}
###
alpha.fn(Portfolio, 1:100)
###
set.seed(7)
alpha.fn(Portfolio, sample(100, 100, replace = T))
###
boot(Portfolio, alpha.fn, R = 1000)
alpha.fn(Portfolio, sample(100, 100, replace = T))
###
boot(Portfolio, alpha.fn, R = 1000)
###
set.seed(7)
alpha.fn(Portfolio, sample(100, 100, replace = T))
alpha.fn(Portfolio, sample(100, 100, replace = T))
alpha.fn(Portfolio, sample(100, 100, replace = T))
alpha.fn(Portfolio, sample(100, 100, replace = T))
###
boot(Portfolio, alpha.fn, R = 1000)
###
boot.out <- boot(Portfolio, alpha.fn, R = 1000)
boot.out
## Plot it
plot(boot.out)
###
boot.fn <- function(data, index)
coef(lm(mpg ~ horsepower, data = data, subset = index))
boot.fn(Auto, 1:392)
###
set.seed(1)
boot.fn(Auto, sample(392, 392, replace = T))
boot.fn(Auto, sample(392, 392, replace = T))
###
boot(Auto, boot.fn, 1000)
###
summary(lm(mpg ~ horsepower, data = Auto))$coef
###
boot.fn <- function(data, index)
coef(lm(mpg ~ horsepower, data = data, subset = index))
boot.fn(Auto, 1:392)
###
boot.fn <- function(data, index)
coef(lm(mpg ~ horsepower, data = data, subset = index))
boot.fn(Auto, 1:392)
###
set.seed(1)
boot.fn(Auto, sample(392, 392, replace = T))
boot.fn(Auto, sample(392, 392, replace = T))
###
boot(Auto, boot.fn, 1000)
###
summary(lm(mpg ~ horsepower, data = Auto))$coef
source("~/Google Drive/Temp/Lect 9 (lab)/Lect9_ValidationResampling_Lab.R", echo=TRUE)
source("~/Google Drive/Temp/Lect 9 (lab)/Lect9_ValidationResampling_Lab.R", echo=TRUE)
source("~/Google Drive/Temp/Lect 9 (lab)/Lect9_ValidationResampling_Lab.R", echo=TRUE)
source("~/Google Drive/Temp/Lect 9 (lab)/Lect9_ValidationResampling_Lab.R", echo=TRUE)
###
library(ISLR2)
set.seed(1)
set.seed(1)
train <- sample(392, 196)
train
summary(Auto)
###
lm.fit <- lm(mpg ~ horsepower, data = Auto, subset = train)
train
###
attach(Auto)
(mpg - predict(lm.fit, Auto))
(mpg - predict(lm.fit, Auto))[-train]
(mpg - predict(lm.fit, Auto))[-train]^2
mean((mpg - predict(lm.fit, Auto))[-train]^2)
poly(horsepower, 2)
lm.fit2 <- lm(mpg ~ poly(horsepower, 2), data = Auto,
subset = train)
mean((mpg - predict(lm.fit2, Auto))[-train]^2)
lm.fit3 <- lm(mpg ~ poly(horsepower, 3), data = Auto,
subset = train)
mean((mpg - predict(lm.fit3, Auto))[-train]^2)
###
set.seed(2)
train <- sample(392, 196)
lm.fit <- lm(mpg ~ horsepower, subset = train)
mean((mpg - predict(lm.fit, Auto))[-train]^2)
lm.fit2 <- lm(mpg ~ poly(horsepower, 2), data = Auto,
subset = train)
mean((mpg - predict(lm.fit2, Auto))[-train]^2)
lm.fit3 <- lm(mpg ~ poly(horsepower, 3), data = Auto,
subset = train)
mean((mpg - predict(lm.fit3, Auto))[-train]^2)
###
glm.fit <- glm(mpg ~ horsepower, data = Auto)
coef(glm.fit)
###
lm.fit <- lm(mpg ~ horsepower, data = Auto)
coef(lm.fit)
###
library(boot)
glm.fit <- glm(mpg ~ horsepower, data = Auto)
cv.err <- cv.glm(Auto, glm.fit)
cv.err$delta
cv.err <- cv.glm(Auto, glm.fit)
lm.influence(fit)$h
### Lets write a simple function to compute the short cut formula
loocv=function(fit){
h=lm.influence(fit)$h
mean((residuals(fit)/(1-h))^2)
}
## Now we try it out
cv.glm(Auto, glm.fit)$delta[1]
loocv(glm.fit) # same result but faster
###
cv.error <- rep(0, 10)
cv.error
degree <- 1:10
degree
###
cv.error <- rep(0, 10)
degree <- 1:10
for (d in degree) {
glm.fit <- glm(mpg ~ poly(horsepower, d), data = Auto)
cv.error[d] <- loocv(glm.fit)
}
cv.error
# plot it
plot(degree,cv.error,type="b")
## $k$-Fold Cross-Validation
###
set.seed(17)
cv.error.10 <- rep(0, 10)
for (d in degree) {
glm.fit <- glm(mpg ~ poly(horsepower, d), data = Auto)
cv.error.10[d] <- cv.glm(Auto, glm.fit, K = 10)$delta[1]
}
cv.error.10
lines(degree, cv.error.10, type="b",col="red")
# See bias-correction
cv.glm(Auto, glm.fit, K = 10)$delta
# See bias-correction
cv.glm(Auto, glm.fit, K = 10)$delta
###
alpha.fn <- function(data, index) {
X <- data$X[index]
Y <- data$Y[index]
(var(Y) - cov(X, Y)) / (var(X) + var(Y) - 2 * cov(X, Y))
}
###
alpha.fn <- function(data, index) {
X <- data$X[index]
Y <- data$Y[index]
(var(Y) - cov(X, Y)) / (var(X) + var(Y) - 2 * cov(X, Y))
}
Portfolio
###
alpha.fn(Portfolio, 1:100)
###
set.seed(7)
alpha.fn(Portfolio, sample(100, 100, replace = T))
alpha.fn(Portfolio, sample(100, 100, replace = T))
###
alpha.fn(Portfolio, 1:100)
###
set.seed(7)
alpha.fn(Portfolio, sample(100, 100, replace = T))
alpha.fn(Portfolio, sample(100, 100, replace = T))
###
set.seed(7)
alpha.fn(Portfolio, sample(100, 100, replace = T))
###
boot.out <- boot(Portfolio, alpha.fn, R = 1000)
boot.out
## Plot it
plot(boot.out)
###
boot.fn <- function(data, index)
boot.fn <- function(data, index)
coef(lm(mpg ~ horsepower, data = data, subset = index))
###
boot.fn <- function(data, index)
coef(lm(mpg ~ horsepower, data = data, subset = index))
boot.fn(Auto, 1:392)
###
set.seed(1)
boot.fn(Auto, sample(392, 392, replace = T))
boot.fn(Auto, sample(392, 392, replace = T))
boot.fn(Auto, sample(392, 392, replace = T))
###
boot(Auto, boot.fn, 1000)
###
summary(lm(mpg ~ horsepower, data = Auto))$coef
###
boot.fn <- function(data, index)
coef(
lm(mpg ~ horsepower + I(horsepower^2),
data = data, subset = index)
)
set.seed(1)
boot(Auto, boot.fn, 1000)
###
# OLS is based on restrictive (in this case) assumptions
plot(mpg ~ horsepower)
par(mfrow=c(2,2))
plot(lm(mpg ~ horsepower, data = Auto))
###
library(ISLR2)
Credit
summary(Credit)
install.packages("openai")
library(openai)
# scatter plot car dataset
# write some r code to plot car dataset with ggplot
install.packages(c("igraph", "lattice", "markdown", "Matrix", "quantreg", "RcppArmadillo"))
source("~/Dropbox/GitHub/GARP/GARP_main.R", echo=TRUE)
source("~/Dropbox/GitHub/GARP/GARP_main.R", echo=TRUE)
